---
title: "Conceptual Exercises of Chapter 6"
output: html_notebook
---

# Question 1

(a): Best subset has the smallest training RSS.
(b): It depends. Best subset has more combinations to choose. But all three methods choose candidates according to training RSS, which has no direct relationship with test RSS.

(c):
i: True; ii: True; iii: False; iv: False; v: False.

# Question 2

(a): iii is true. Lasso is more flexible than least squares (see first paragraph of section 6.4.3 on page 241). So its variance decreases compared with least squares. Although the bias increases at the same time, the increase is less than the decrease of variance. So the overall test RSS decreases.

(b): iii is true. The same reason of lasso.

(c): ii is true. The opposite of lasso's reason.

# Question 3

$s$ 是 Figure 6.7中左图中矩形的边长。
$s = 0$ 时，矩形变成点 $(0, 0)$，除了截距 $\beta_0 \neq 0$，$\beta_i = 0, i = 1..p$，拟合曲线变成了一条水平线；此时 variance 为 0（不论 training set 如何变化，$\beta_i$ 不发生任何变化），$(bias)^2$ 最大；

为什么$\lambda$ 与 $s$ 的变化方向是相反的：$\lambda$ 越大，式 (6.7) 第2项 $\lambda \sum_{j=1}^p \vert \beta_j \vert$ 的影响就越大，对 $\sum_{j=1}^p \vert \beta_j \vert$ 施加的要求更大，要求它的上限值比较小，也就是 $s$ 比较小。

随着 $s$ 增加：

(a): 根据图 6.7，training RSS 不断减小，当 $s$ 所代表的四边形覆盖 $\hat\beta$ 时达到最小值，$s$继续增大时 training RSS 不再继续降低；

(b): $s=0$ 时 test RSS 比较大，随着 $s$ 增加，test RSS 开始降低，随着 $s$ 越来越大，对 $\beta_i$的约束越来越小，出现过拟合后 test RSS 开始增加，整体呈 U 型，参考图 (6.8)；

(c): variance 在$s=0$时为0，随$s$增加而增加，增加到 least squares 对应的 variance 时不再增加，参考图(6.8)；

(d): Squared bias 随 $s$ 增加而降低，降低到 least squares对应的 bias 时不在降低，参考图(6.8)；

(e): Irreducible error 不随 $s$ 变化。

# Question 4

$\lambda$ 与 $s$ 的变化趋势相反（理由见上一题解答），$\lambda$ 不断增加的过程就是 Figure 6.7中右图圆半径不断减小的过程，所以本题中各量变化趋势与上题相反，初始状态相当于 least squares，随着 $\lambda$ 增加：

(a): Training RSS 持续增加到 $\beta_i = 0, i=1..p$ 后不再变化；

(b): 随着过拟合的较少而降低，过拟合消失后随着欠拟合的出现转而升高，呈U型变化；

(c): variance 不断降低；

(d): Squared bias 不断增加；

(e): Irreducible error 不变。

# Question 5

## (a) & (b)

根据已知条件，设 $x_{11} = x, y_1 = y$，则问题可表示为：
$$
\begin{pmatrix} x & x \\ -x & -x \end{pmatrix}
\begin{pmatrix} \hat\beta_1 \\ \hat\beta_2 \end{pmatrix} + \hat\beta_0
= \begin{pmatrix} y \\ -y \end{pmatrix}
$$

展开上式得到以 $x,y$ 为自变量的二元一次方程组，二式相加，得到 $\hat\beta_0 = 0$，所以上式简化为：
$$
\begin{pmatrix} x & x \\ -x & -x \end{pmatrix}
\begin{pmatrix} \hat\beta_1 \\ \hat\beta_2 \end{pmatrix}
= \begin{pmatrix} y \\ -y \end{pmatrix}
\tag{1} \label{eq1}
$$
Ridge regression优化问题可以表示为：
求 $\hat\beta_1, \hat\beta_2$，使得 $M_R$ 取得最小值，其中 $M_R = RSS + \lambda \sum_{j=1}^p \beta_j^2$，将式 $\eqref{eq1}$ 代入式(6.5)，得到 $M_R$ 的表达式：
$$
M_R = 2 [y - (\hat\beta_1 + \hat\beta_2)x] ^ 2 + \lambda (\hat\beta_1^2 + \hat\beta_2^2)
\tag{2} \label{eq2}
$$

$M_R$ 取最小值时有：
$$
\begin{cases}
\frac{\partial M_R}{\partial \hat\beta_1} = 0 \\
\frac{\partial M_R}{\partial \hat\beta_2} = 0
\end{cases}
$$

将式 $\eqref{eq2}$ 代入上式得：
$$
\begin{cases}
-4xy + 4x^2(\hat\beta_1 + \hat\beta_2) + 2\lambda\hat\beta_1 = 0 \\
-4xy + 4x^2(\hat\beta_1 + \hat\beta_2) + 2\lambda\hat\beta_2 = 0
\end{cases}
$$

两式相减得：$\hat\beta_1 = \hat\beta_2$.

## (c) & (d)

将式 $\eqref{eq1}$ 代入式(6.7)，得到Lasso优化目标$minimize_{\beta}\{M_L\}$，其中：
$$
M_L = 2 [y - (\hat\beta_1 + \hat\beta_2)x] ^ 2 + \lambda(\vert \hat\beta_1 \vert + \vert \hat\beta_2 \vert)
\tag{3} \label{eq3}
$$
$M_L$ 取最小值时有：
$$
\begin{cases}
\frac{\partial M_L}{\partial \hat\beta_1} = 0 \\
\frac{\partial M_L}{\partial \hat\beta_2} = 0
\end{cases}
$$
将式 $\eqref{eq3}$ 代入上式，当$\hat\beta_1 \gt 0, \hat\beta_2 \gt 0$时有：
$$
\begin{cases}
-4xy + 4x^2(\hat\beta_1 + \hat\beta_2) + \lambda = 0 \\
-4xy + 4x^2(\hat\beta_1 + \hat\beta_2) + \lambda = 0
\end{cases} \\
\therefore \hat\beta_1 + \hat\beta_2 = \frac{4xy - \lambda}{4x^2}
$$

当$\hat\beta_1 \lt 0, \hat\beta_2 \lt 0$时有：
$$
\begin{cases}
-4xy + 4x^2(\hat\beta_1 + \hat\beta_2) - \lambda = 0 \\
-4xy + 4x^2(\hat\beta_1 + \hat\beta_2) - \lambda = 0
\end{cases} \\
\therefore \hat\beta_1 + \hat\beta_2 = \frac{4xy + \lambda}{4x^2}
$$

当$\hat\beta \hat\beta_2 \lt 0$时有：
$$
\begin{cases}
-4xy + 4x^2(\hat\beta_1 + \hat\beta_2) + \lambda = 0 \\
-4xy + 4x^2(\hat\beta_1 + \hat\beta_2) - \lambda = 0
\end{cases} \\
\therefore \hat\beta_1 + \hat\beta_2 = \frac yx
$$
以上3种情况下 $\hat\beta_1, \hat\beta_2$ 都有无穷多解，所以使用Lasso方法有无穷多解。
