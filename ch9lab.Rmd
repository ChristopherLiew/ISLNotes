---
title: "Lab of Chapter 9"
output: html_notebook
---

# 9.6.1 Support Vector Classifier

## For data not linearly seperable

Setup data set:
```{r}
library(e1071)
set.seed(1)
x <- matrix(rnorm(20 * 2), ncol = 2)
y <- c(rep(-1, 10), rep(1, 10))
x[y == 1, ] <- x[y == 1, ] + 1
```

这里从 `x` 数据集中选择了一半观测点, 给这些点的x坐标、y坐标分别加1，相当于将这些点向右上方平移了 $\sqrt 2$。

Check whether the classes are linearly separable:
```{r}
plot(x, col = (3 - y))
```

Fit the support vector classifier and plot:
```{r}
dat <- data.frame(x = x, y = as.factor(y))
svmfit <- svm(y ~ ., data = dat, kernel = 'linear', cost = 10, scale = FALSE)
plot(svmfit, dat)
```

由于这幅图将x2作为横坐标，x1作为纵坐标，直接看与上面原始数据点图对应不上，但只要把原始数据点图坐标变换到上图的方向（需要在图片浏览软件里先旋转，再翻转），就会发现20个数据点都能对应上。

上图中，青色和紫色的边界就是超平面所在位置（由于构建svm时指定了 `kernel = 'linear'`，所以边界是直线，由于绘图算法的原因，图中是一条有毛刺的直线），叉号代表支持向量，圆圈代表非支持向量，所以叉号覆盖的、与超平面平行的带状区域，就是margin所在区域（margin的含义可参考P342上的图9.3）。
在 (x1 = -1.2, x2 = 1) 位置上的红色叉号处于青色区域内，表示这个点被错误的分类了。

Summary of the model:
```{r}
svmfit$index
summary(svmfit)
```
`(4 3)` 表明上面的7个支持向量，4个属于 -1 类（黑色，都位于青色区域中），3个属于 1 类（红色，其中2个位于紫色区域中，1个位于青色区域中，表示被错误的分类），与上图中青色和紫色区域中的叉号分布吻合。

Build support vector classifier with smaller cost:
```{r}
svmfit <- svm(y ~ ., data = dat, kernel = 'linear', cost = 0.1, scale = FALSE)
plot(svmfit, dat)
svmfit$index
```

与 cost = 10 的输出图相比，margin 明显变宽了，覆盖了更多的数据点成为 support vector，由于更多的数据点参与了超平面的确定，可以认为超平面对数据变化的敏感度降低了，也就是variance降低了(bias升高了)。


Choose cost value with cross-validation:
```{r}
set.seed(1)
tune.out <- tune(svm, y ~., data = dat, kernel = 'linear', ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
```

Summary the best model:
```{r}
best.model <- tune.out$best.model
summary(best.model)
```

Calculate the test error.
First build the test dataset:
```{r}
xtest <- matrix(rnorm(20 * 2), ncol = 2)
ytest <- sample(c(1, -1), 20, replace = TRUE)
xtest[ytest == 1, ] = xtest[ytest == 1, ] + 1
testdat <- data.frame(x = xtest, y = as.factor(ytest))
```

Predict the class on the test dataset:
```{r}
ypred <- predict(best.model, testdat)
table(predict = ypred, truth = testdat$y)
```

The correct rate is: (7 + 9) / (7 + 4 + 0 + 9) = 80%.

Predict based on the model with cost = 0.01:
```{r}
svmfit <- svm(y ~ ., data = dat, kernel = 'linear', cost = 0.01, scale = FALSE)
ypred <- predict(svmfit, testdat)
table(predict = ypred, truth = testdat$y)
```

The correct rate is (7 + 4) / (7 + 4 + 0 + 9) = 55%, which is lower than the model with cost 0.1.

## For data linearly seperable

Move the half data point more faraway:
```{r}
x[y == 1, ] = x[y == 1, ] + 0.5
plot(x, col = (y + 5) / 2, pch = 19)
```

这里生成的数据向右上平移0.5后仍然不可分，所以上面的代码将平移量改成了1。

Build svm:
```{r}
dat <- data.frame(x = x, y = as.factor(y))
svmfit <- svm(y ~ ., data = dat, kernel = 'linear', scale = FALSE, cost = 1e5)
summary(svmfit)
plot(svmfit, data = dat)
```
可以看到高 cost 导致 margin非常窄。

Build the svm with lower cost:
```{r}
svmfit <- svm(y ~ ., data = dat, kernel = 'linear', scale = FALSE, cost = 1)
summary(svmfit)
plot(svmfit, data = dat)
```

降低 cost 后，margin 宽了许多，其中一个数据点 (x1 = -0.5, x2 = 1.5) 被划分到错误的区域中。所以 bias 比 cost = 1e5 时高，相应地 variance 降低了。

# 9.6.2 Support Vector Machine

Generate some data with a non-linear class boundary:
```{r}
set.seed(1)
x <- matrix(rnorm(200 * 2), ncol = 2)
x[1:100, ] <- x[1:100, ] + 2
x[101:150, ] <- x[101:150, ] - 2
y <- c(rep(1, 150), rep(2, 50))
dat <- data.frame(x = x, y = as.factor(y))
plot(x, col = y)
```

前100个点向右上平移，第101 ~ 150个点向左下平移，这两部分标记为1，中间夹的未移动的50个点标记为2。

Split the data into training and test set, and build a SVM (with $\gamma = 1$) on the training set:
```{r}
train <- sample(200, 100)
svmfit <- svm(y ~ ., data = dat[train, ], kernel = 'radial', gamma = 1, cost = 1)
plot(svmfit, data = dat[train, ])
summary(svmfit)
```

Fit with a large cost:
```{r}
svmfit <- svm(y ~ ., data = dat[train, ], kernel = 'radial', gamma = 1, cost = 1e5)
plot(svmfit, data = dat[train, ])
```

Select best $\gamma$ and cost with cross-validation:
```{r}
set.seed(1)
tune.out <- tune(svm, y ~ ., data = dat[train, ], kernel = 'radial', ranges = list(cost = c(0.1, 1, 10, 100, 1000), gamma = c(0.5, 1, 2, 3, 4)))
summary(tune.out)
```

Apply the best model on the test set:
```{r}
table(prediction = predict(tune.out$best.model, newdata = dat[-train, ]), truth = dat[-train, ]$y)
```

So the correct rate is:
```{r}
(68 + 21) / (68 + 1 + 10 + 21)
```

# 9.6.3 ROC Curves

Define a ROC plot function:
```{r}
library(ROCR)
rocplot <- function(pred, truth, ...) {
  predob <- prediction(pred, truth)
  perf <- performance(predob, 'tpr', 'fpr')
  plot(perf, ...)
}
```

*Decision values* 是将计算出的系数代入到观测值向量后得到的值，超平面两侧的观测点各自的 *decision values* 分别大于和小于零，绝对值越大，离分隔面越远，当这个值为0时，表示观测点在分隔面上。

In order to obtain the fitted values for a given SVM model fit, we use `decision.values=TRUE` when fitting svm() . Then the `predict()` function will output the fitted values:
```{r}
svmfit.opt <- svm(y ~ ., data = dat[train, ], kernel = 'radial', cost = 1, gamma = 2, decision.values = TRUE)
fit <- attributes(predict(svmfit.opt, dat[train, ], decision.values = TRUE))$decision.values
par(mfrow = c(1,2))
rocplot(fit, dat[train, ]$y, main = 'Training Data')

# By increasing $\gamma$ we can produce a more flexible fit and generate further improvements in accuracy:

svmfit.flex <- svm(y ~ ., data = dat[train, ], kernel = 'radial', cost = 1, gamma = 50, decision.values = TRUE)
fit <- attributes(predict(svmfit.flex, dat[train, ], decision.values = TRUE))$decision.values
rocplot(fit, dat[train, ]$y, add = TRUE, col = 'red')
```

注意这里不能把增加 gamma 后的代码放在新的 code chunk 里，不同 chunk 间的 plot 对象是不共享的。

Predict and plot on test dataset:
```{r}
fit <- attributes(predict(svmfit.opt, dat[-train, ], decision.values = TRUE))$decision.values
rocplot(fit, dat[-train, ]$y, main = 'Test Data')
fit <- attributes(predict(svmfit.flex, dat[-train, ], decision.values = TRUE))$decision.values
rocplot(fit, dat[-train, ]$y, add = TRUE, col = 'red')
```

So the model `svmfit.opt` with `gamma = 2` is better than the model `svmfit.flex` with `gamma = 50`.

# 9.6.4 SVM with Multiple Classes

```{r}
set.seed(1)
x <- rbind(x, matrix(rnorm(50 * 2),  ncol = 2))
y <- c(y, rep(0, 50))
x[y == 0, 2] <- x[y == 0, 2] + 2
dat <- data.frame(x = x, y = as.factor(y))
par(mfrow = c(1,1))
plot(x, col = (y + 1))
```

可以看到黑色(y = 0)数据点相对于原始位置（绿色点所在区域）向上平移了2个单位（只给y坐标加了2）。

`plot()` 函数的 `col` 参数自带8种颜色，超过8后重新开始循环：
```{r}
lst <- c(1:17)
barplot(lst, axes=FALSE, col=lst)
```

Fit an SVM to the data:
```{r}
svmfit <- svm(y ~ ., data = dat, kernel = 'radial', gamma = 1, cost = 10)
plot(svmfit, dat)
```

再次提醒此图的坐标轴与上面的数据分布图不同，需要旋转90度再水平翻转才能与数据分布图对照。

响应变量的水平数为2时，使用标准 SVM 分类，如果大于2，使用 one-vs-one 方法（见9.4.1节）确定观测点的响应变量类型。

# 9.6.5 Application to Gene Expression Data

```{r}
library(ISLR)
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
table(Khan$ytrain)
```

So there are 63 observations in training set and 20 observations in test set, each with 2308 features. The response variable has 4 levels, marked as 1 ~ 4.

Fit a SVM with a linear kernel on training set:
```{r}
dat <- data.frame(x = Khan$xtrain, y = as.factor(Khan$ytrain))
out <- svm(y ~ ., data = dat, kernel = 'linear', cost = 10)
summary(out)
table(out$fitted, dat$y)
```

非对角线上全部是0，也就是没有 training bias，由于特征数大大高于观测数，这个结果是合理的。

Predict on the test set:
```{r}
pred <- predict(out, Khan$xtest)
table(pred, Khan$ytest)
```

非对角线上有2个观测：有两个属于3类的观测被预测为了2类。
