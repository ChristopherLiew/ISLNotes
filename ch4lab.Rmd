---
title: "Lab of Chapter 4"
output: html_notebook
---

# 4.6.1 The Stock Market Data
```{r}
library(ISLR)
head(Smarket)
cor(Smarket[,-9])
plot(Smarket$Volume)
```

# 4.6.2 Logistic Regression

```{r}
f2 <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data=Smarket, family=binomial)
summary(f2)
coef(f2)
summary(f2)$coef
summary(f2)$coef[,4]
```

Predict response based on predictors:
```{r}
gprob <- predict(f2, type = 'response')
gprob[1:10]
contrasts(Smarket$Direction)
```

Generate confusion matrix:
```{r}
gpred <- rep('Down', 1250)
gpred[gprob > 0.5] <- 'Up'
table(gpred, Smarket$Direction)
```

Calculate the training error rate on all training set:
```{r}
(145 + 507) / 1250
mean(gpred == Smarket$Direction)
```

Split existing training set to training and test subset:
```{r}
train <- (Smarket$Year < 2005)
sm2005 <- Smarket[!train,]
dim(sm2005)
direction2005 <- sm2005$Direction
```

Build a logistic regression model on training set:
```{r}
f2.2 <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, 
            data=Smarket, family=binomial, subset = train)
```

With above model, predict response on test set, and calculating test error rate:
```{r}
gprob2005 <- predict(f2.2, sm2005, type = 'response')
gpred2005 <- rep('Down', 252)
gpred2005[gprob2005 > 0.5] <- 'Up'
table(gpred2005, direction2005)
# mean(gpred2005 == direction2005)
mean(gpred2005 != direction2005)
```

Predict again with removing irrelevant predictors:
```{r}
f2.p2 <- glm(Direction ~ Lag1 + Lag2, data=Smarket, family = binomial, subset=train)
gprob2005.p2 <- predict(f2.p2, sm2005, type="response")
gpred2005.p2 <- rep("Down", 252)
gpred2005.p2[gprob2005.p2 > 0.5] <- "Up"
table(gpred2005.p2, direction2005)
mean(gpred2005.p2 == direction2005)
```

Predict the returns associated with particular values of *Lag1* and *Lag2*:
```{r}
predict(f2.p2, newdata = data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)), type = 'response')
```

# 4.6.3 Linear Discriminant Analysis
```{r}
library(MASS)
ldaf <- lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
ldaf
plot(ldaf)
```

How to read this plot?
The y-axis is the probability of *Down* and *Up*.
What does the x-axis stand for?

```{r}
lda.pred <- predict(ldaf, sm2005)
names(lda.pred)
lda.class <- lda.pred$class
table(lda.class, direction2005)
mean(lda.class == direction2005)
sum(lda.pred$posterior[,1] > 0.5)
sum(lda.pred$posterior[,1] < 0.5)
lda.pred$posterior[1:20,1]
lda.class[1:20]
sum(lda.pred$posterior[,1] > 0.9)
```

# 4.6.4 Quadratic Discriminant Analysis

```{r}
qdaf <- qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
qdaf
qda.class <- predict(qdaf, sm2005)$class
table(qda.class, direction2005)
mean(qda.class == direction2005)
```

Compared with linear discriminant analysis, the accurate rate increased from 0.56 to 0.59.

# 4.6.5 K-Nearest Neighbours

Predict stock market direction wth $k=1$:
```{r}
library(class)
train.X <- cbind(Smarket$Lag1, Smarket$Lag2)[train,]
test.X <- cbind(Smarket$Lag1, Smarket$Lag2)[!train,]
train.direction <- Smarket$Direction[train]
set.seed(1)
knn.pred <- knn(train.X, test.X, train.direction, k = 1)
table(knn.pred, direction2005)
mean(knn.pred == direction2005)
```

Predict stock market direction wth $k=3$:
```{r}
library(class)
knn.pred3 <- knn(train.X, test.X, train.direction, k = 3)
table(knn.pred3, direction2005)
mean(knn.pred3 == direction2005)
```

# 4.6.6 An Application to Caravan Insurance Data

Load dataset *Caravan* and standardized it:
```{r}
head(Caravan)
str(Caravan)
std.X <- scale(Caravan[, -86])
var(Caravan[,1])
var(std.X[,1])
```

Split train and test data:
```{r}
test <- 1:1000
train.X <- std.X[-test,]
test.X <- std.X[test,]
train.Y <- Caravan$Purchase[-test]
test.Y <- Caravan$Purchase[test]
set.seed(1)
knn.pred <- knn(train.X, test.X, train.Y, k=1)
mean(knn.pred != test.Y)
mean(test.Y != 'No')
table(knn.pred, test.Y)
```

$9 \div (68+9) \approx 0.117$, which is success rate based on KNN prediction, is double the rate from random guessing (0.059).

Predict with $k=3$ and 5:
```{r}
knn.pred3 <- knn(train.X, test.X, train.Y, k=3)
table(knn.pred3, test.Y)
knn.pred5 <- knn(train.X, test.X, train.Y, k=5)
table(knn.pred5, test.Y)
```

$5 \div (21 + 5) \approx 0.192$, and $4 \div (11 + 4) \approx 0.267$, are much higher than the result of $k=1$.

Logistic regression predict with cut-off 0.5:
```{r}
lr.fit <- glm(Purchase ~ ., data = Caravan, family = 'binomial', subset = -test)
purchase.prob <- predict(lr.fit, Caravan[test,], type = 'response')
purchase.res <- rep('No', 1000)
purchase.res[purchase.prob > 0.5] = 'Yes'
table(purchase.res, test.Y)
```

All 7 "purchased" prediction are all wrong!
Change threshold to 0.25:
```{r}
purchase.res[purchase.prob > 0.25] = 'Yes'
table(purchase.res, test.Y)
```

The success rate is $11 \div (22 + 11) \approx 0.333$.