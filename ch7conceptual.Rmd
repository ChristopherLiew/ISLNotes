---
title: "Conceptual Exercises of Chapter 7"
output: html_notebook
---

# Question 1

## (a)
When $x \le \xi$: 
$$
a_1 = \beta_0 \\
b_1 = \beta_1 \\
c_1 = \beta_2 \\
d_1 = \beta_3
$$

## (b)
When $x \lt \xi$:
$$
a_2 = \beta_0 - \beta_4 \xi^3 \\
b_2 = \beta_1 + 3 \beta_4 \xi^2 \\
c_2 = \beta_2 - 3 \beta_4 \xi \\
d_2 = \beta_3 + \beta_4
$$

## (c)
When $x = \xi$:
$$
f_1(\xi) = \beta_0 + \beta_1 \xi + \beta_2 \xi^2 + \beta_3 \xi^3 \\
f_2(\xi)
= \beta_0 - \beta_4 \xi^3 + (\beta_1 + 3 \beta_4 \xi^2) \xi 
+ (\beta_2 - 3 \beta_4 \xi) \xi^2 + (\beta_3 + \beta_4) \xi^3
= \beta_0 + \beta_1 \xi + \beta_2 \xi^2 + \beta_3 \xi^3 \\
\therefore f_1(\xi) = f_2(\xi)
$$

## (d)
When $x = \xi$:
$$
f_1'(x) = b_1 + 2c_1x + 3d_1x^2 \\
\therefore f_1'(\xi) = \beta_1 + 2\beta_2\xi + 3\beta_3 \xi^2 \\
f_2'(x) = b_2 + 2c_2 x + 3d_2 x^2 \\
\therefore f_2'(\xi)
= \beta_1 + 3 \beta_4 \xi^2 + 2 (\beta_2 - 3 \beta_4 \xi) \xi + 3(\beta_3 + \beta_4)\xi^2
= \beta_1 + 2\beta_2\xi + 3\beta_3 \xi^2 \\
= f_1'(\xi)
$$

## (e)
When $x = \xi$:
$$
f_1''(x) = 2c_1 + 6d_1x \\
\therefore f_1''(\xi) = 2\beta_2\ + 6\beta_3 \xi \\
f_2''(x) = 2c_2 + 6d_2 x \\
\therefore f_2''(\xi)
= 2(\beta_2 - 3 \beta_4 \xi) + 6 (\beta_3 + \beta_4) \xi = 2\beta_2\ + 6\beta_3 \xi\\
= f_1''(\xi)
$$

# Question 2

By definition, $\hat g(x)$ is always positive or zero.
So in any scenario, $\hat g(x)$ is a curve which is as close to $y = 0$ as possible.

(a): $\hat g(x) = 0$

(b): $\hat g(x) = k$

(c): $\hat g(x) = ax + b$

(d): $\hat g(x) = ax^2 + bx + c$

(e): when $\lambda = 0$, no matter what the value of $m$, $\hat g(x)$ is always the linear regression curve based on the $n$ points.

# Question 3

Take $b_1(X)$, $b_2(X)$, $\hat\beta_0$, $\hat\beta_1$, $\hat\beta_2$ into expression of $Y$, we have:
$$
Y = \begin{cases}
1 + X & X \le 1 \\
1 + X - 2 (X - 1)^2 & X > 1
\end{cases}
$$

So the curve is as follows:
```{r}
x1 <- seq(-2, 1, by = 0.01)
x2 <- seq(1.01, 2, by = 0.01)
y1 <- 1 + x1
y2 <- 1 + x2 - 2 * (x2 -1) ^ 2
plot(c(x1, x2), c(y1, y2), type = 'l')
points(-2, -1, col = 'red', pch = 19)
points(1, 2, col = 'red', pch = 19)
points(2, 1, col = 'red', pch = 19)
```

Or more simple:
```{r}
x <- seq(-2, 2, by = 0.1)
y <- 1 + x - 2 * (x - 1) ^ 2 * I(x > 1)
plot(x, y, type = 'l')
```

# Question 4

Take $b_1$, $b_2$ and $\beta_0, \dots, \beta_2$ into expression of $Y$:
```{r}
x <- -2 : 2
y <- 1 + I(x >= 0 & x <= 2) - (x - 1) * I(x >=1 & x <= 2) + 3 * (x - 3) * I(x >= 3 & x <= 4) + 3 * I(x > 4 & x <= 5)
plot(x, y, type = 'l')
```

# Question 5

(a): As $\lambda \to \infty$, $\hat g_2$ has smaller training RSS, because it's more flexible than $\hat g_1$.

(b): As $\lambda \to \infty$, when there's no overfit, $\hat g_2$ has smaller test RSS than $\hat g_1$. Otherwise $\hat g_1$ has smaller test RSS than $\hat g_2$.

(c): When $\lambda = 0$, $\hat g_1 = \hat g_2$. So they have same RSS on training and test data set.