---
title: "Applied Exercises of Chapter 7"
output: html_notebook
---

# Question 6

## (a)

```{r}
library(ISLR)
library(boot)
set.seed(1)
degree <- 10
cv.errs <- rep(NA, degree)
for (i in 1:degree) {
  fit <- glm(wage ~ poly(age, i), data = Wage)
  cv.errs[i] <- cv.glm(Wage, fit)$delta[1]
}
```
Plot the test MSE by the degrees:
```{r}
plot(1:degree, cv.errs, xlab = 'Degree', ylab = 'Test MSE', type = 'l')
deg.min <- which.min(cv.errs)
points(deg.min, cv.errs[deg.min], col = 'red', cex = 2, pch = 19)
```

The minimum of test MSE at the degree 9. But test MSE of degree 4 is small enough.
The comparison by ANOVA (`anova(fit.1,fit.2,fit.3,fit.4,fit.5)`, on page 290, section 7.8.1) suggests degree 4 is enough.

Predict with 3 degree model:
```{r}
plot(wage ~ age, data = Wage, col = "darkgrey")
age.range <- range(Wage$age)
age.grid <- seq(from = age.range[1], to = age.range[2])
fit <- lm(wage ~ poly(age, 3), data = Wage)
preds <- predict(fit, newdata = list(age = age.grid))
lines(age.grid, preds, col = "red", lwd = 2)
```

## (b)

```{r}
cv.errs <- rep(NA, degree)
for (i in 2:degree) {
  Wage$age.cut <- cut(Wage$age, i)
  fit <- glm(wage ~ age.cut, data = Wage)
  cv.errs[i] <- cv.glm(Wage, fit)$delta[1]
}
plot(2:degree, cv.errs[-1], xlab = 'Cuts', ylab = 'Test MSE', type = 'l')
deg.min <- which.min(cv.errs)
points(deg.min, cv.errs[deg.min], col = 'red', cex = 2, pch = 19)
```

So 8 cuts produce minimum test MSE.

Predict with 8-cuts step function:
```{r}
plot(wage ~ age, data = Wage, col = "darkgrey")
fit <- glm(wage ~ cut(age, 8), data = Wage)
preds <- predict(fit, data.frame(age = age.grid))  # both `data.frame` and `list` work
lines(age.grid, preds, col = "red", lwd = 2)
```

Understand the `cut()` function:
```{r}
res <- cut(c(1,5,2,3,8), 2)
res
length(res)
class(res[1])
```

`cut(x, k)` acts like *bin* or *binage*, turning a continuous quantitative variable into a discrete qualitative variable, by deviding the range of `x` evenly into `k` intervals.
Each interval is called a *level*.
The output of `cut(x, k)` is a vector with the same length of `x`.
Each element of output (a *factor* object) is a *level* where the corresponding input element falls in.

# Question 7

See introductions about regression on qulitative predictors in section 3.3.1 and 3.6.6.

Use `summary()`
```{r}
set.seed(1)
summary(Wage$maritl)
# table(Wage$maritl) the same with `summary`
summary(Wage$jobclass)
par(mfrow = c(1, 2))
plot(Wage$maritl, Wage$wage)
plot(Wage$jobclass, Wage$wage)
```

Fit wage on multiple predictors with GAM:
```{r}
library(gam)
fit1 <- gam(wage ~ lo(year, span = 0.7) + s(age, 5) + education, data = Wage)
fit2 <- gam(wage ~ lo(year, span = 0.7) + s(age, 5) + education + jobclass, data = Wage)
fit3 <- gam(wage ~ lo(year, span = 0.7) + s(age, 5) + education + maritl, data = Wage)
fit4 <- gam(wage ~ lo(year, span = 0.7) + s(age, 5) + education + jobclass + maritl, data = Wage)
anova(fit1, fit2, fit3, fit4)
```

So model *fit4* fits the best.

Plot the model:
```{r}
par(mfrow = c(2, 3))
plot(fit4, se = T, col = "blue")
```

# Question 8

There's no direct way to answer if there are nonlinear realtionships between any feature in the data set. We have to choose a feature as the response, some others as predictors. And investigate if the relationship between them is linear or not.

Get the overall relationship between all pairs of the *Auto* data set:
```{r}
set.seed(1)
pairs(Auto)
```

From the plot above we can see when using *mpg* as the response, there are clear relationships between it and *cylinders*, *displacement*, *horsepower*, *weight*. Now we will find out if they are nonlinear or not.

```{r}
fit <- lm(mpg ~ poly(cylinders, 2) + poly(displacement, 5) + poly(horsepower, 5) + poly(weight, 5), data = Auto)
summary(fit)
```

The results show that there's no significant relationship between mpg and cylinders.
There's weak relationship (p-value: 0.028) between mpg and displacement.
There's strong quadratic relation between mpg and horsepower.
There's strong linear relation between mpg and weight.

Test the degree with ANOVA:
```{r}
anv1 <- gam(mpg ~ displacement + horsepower + weight, data = Auto)
anv2 <- gam(mpg ~ displacement + s(horsepower, 2) + weight, data = Auto)
anv3 <- gam(mpg ~ s(displacement, 5) + s(horsepower, 5) + s(weight, 5), data = Auto)
anova(anv1, anv2, anv3, test = 'F')
summary(anv3)
par(mfrow=c(1,3))
plot.Gam(anv3, se=TRUE, col="red")
```

According to plot of *anv3*, try quadratic with *displacement* and *horsepower*, linear with *weight*:
```{r}
anv4 <- gam(mpg ~ s(displacement, 3) + s(horsepower, 3) + weight, data = Auto)
anova(anv4, anv3, test = 'F')
par(mfrow=c(1,3))
plot(anv4, se=TRUE, col="red")
```

So model *anv4* is good enough.

Compare their test MSE:
```{r}
lm1 <- glm(mpg ~ displacement + horsepower + weight, data = Auto)
lm2 <- glm(mpg ~ poly(displacement, 3) + poly(horsepower, 3) + weight, data = Auto)
lm3 <- glm(mpg ~ poly(displacement, 5) + poly(horsepower, 5) + poly(weight, 5), data = Auto)

cv.glm(Auto, lm1, K = 10)$delta[1]
cv.glm(Auto, lm2, K = 10)$delta[1]
cv.glm(Auto, lm3, K = 10)$delta[1]
```

The results also suggest model *lm2* (same with *anv4*) is good enough.

So the conclusion of relationships with *mpg*:
mpg ~ displacement: cubic;
mpg ~ horsepower: cubic;
mpg ~ weight: linear.

# Question 9

This question uses the variables *dis* (the weighted mean of distances to five Boston employment centers) and *nox* (nitrogen oxides concentration in parts per 10 million) from the *Boston* data. We will treat *dis* as the predictor and *nox* as the response.

## (a)

Use the `poly()` function to fit a cubic polynomial regression to predict *nox* using *dis*. Report the regression output, and plot the resulting data and polynomial fits.

```{r}
library(MASS)
fit <- lm(nox ~ poly(dis, 3), data = Boston)
summary(fit)
dis.grid <- seq(min(Boston$dis), max(Boston$dis), by = 0.1)
preds <- predict(fit, list(dis = dis.grid), se = TRUE)
se.bands <- cbind(preds$fit + 2* preds$se.fit, preds$fit - 2 * preds$se.fit)
plot(nox ~ dis, data = Boston, col = "darkgrey")
lines(dis.grid, preds$fit, lwd = 2, col = "red")
matlines(dis.grid, se.bands, lwd = 1, col = "red", lty = 3)
```

Another way to plot the fit:
```{r}
preds <- predict(fit, list(dis = dis.grid))
plot(nox ~ dis, data = Boston, col = "darkgrey")
lines(dis.grid, preds, col = "red", lwd = 2)
```

Yet another way to plot the fit:
```{r}
plot(Boston$dis, Boston$nox)
points(Boston$dis, fit$fitted.values, col = 'red')
```

## (b)

Plot the polynomial fits for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares.

```{r}
rss <- rep(NA, 10)
for (i in 1:10) {
  fit <- lm(nox ~ poly(dis, i), data = Boston)
  rss[i] <- sum(fit$residuals ^ 2)
}
plot(1:10, rss, type = 'l', xlab = "Degree", ylab = "RSS")
```

## (c)

Perform cross-validation or another approach to select the optimal degree for the polynomial, and explain your results.

```{r}
testMSE <- rep(NA, 10)
for (i in 1:10) {
  fit <- glm(nox ~ poly(dis, i), data = Boston)
  testMSE[i] <- cv.glm(Boston, fit, K = 10)$delta[1]
}
plot(1:10, testMSE, type = 'l', xlab = "Degree", ylab = "Test MSE")
points(which.min(testMSE), testMSE[which.min(testMSE)], col = 'red', pch = 19)
```

So the optimal degree is 3.

## (d)

Use the `bs()` function to fit a regression spline to predict *nox* using *dis*. Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting fit.

```{r}
library(splines)
dof <- 4
fit <- lm(nox ~ bs(dis, df = dof), data = Boston)
attr(bs(Boston$dis, df = dof), "knots")

preds <- predict(fit, list(dis = dis.grid), se = TRUE)
se.bands <- cbind(preds$fit + 2* preds$se.fit, preds$fit - 2 * preds$se.fit)
plot(nox ~ dis, data = Boston, col = "darkgrey")
lines(dis.grid, preds$fit, lwd = 2, col = "red")
matlines(dis.grid, se.bands, lwd = 1, col = "red", lty = 3)
```

For degree of freedom (*dof*) equals to the sum of the number of knots and degree, and the default degree is 3, there is 1 knot.

## (e)

Now fit a regression spline for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. Describe the results obtained.

```{r}
res <- c()
df.range <- 3:16
for (dof in df.range) {
  fit <- lm(nox ~ bs(dis, df = dof), data = Boston)
  res <- c(res, sum(fit$residuals ^ 2))
}
plot(df.range, res, type = 'l', xlab = 'degree of freedom', ylab = 'RSS')
```

From the results, `df = 10` is good enough.

## (f)

Perform cross-validation or another approach in order to select the best degrees of freedom for a regression spline on this data. Describe your results.

```{r}
res <- c()
for (dof in df.range) {
  fit <- glm(nox ~ bs(dis, df = dof), data = Boston)
  testMSE <- cv.glm(Boston, fit, K = 10)$delta[1]
  res <- c(res, testMSE)
}
plot(df.range, res, type = 'l', xlab = 'degree of freedom', ylab = 'Test MSE')
points(which.min(res) + 2, res[which.min(res)], col = 'red', pch = 19)
```

According to test MSE, `df = 8` is a good choice.