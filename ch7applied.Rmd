---
title: "Applied Exercises of Chapter 7"
output: html_notebook
---

# Question 6

## (a)

```{r}
library(ISLR)
library(boot)
set.seed(1)
degree <- 10
cv.errs <- rep(NA, degree)
for (i in 1:degree) {
  fit <- glm(wage ~ poly(age, i), data = Wage)
  cv.errs[i] <- cv.glm(Wage, fit)$delta[1]
}
```
Plot the test MSE by the degrees:
```{r}
plot(1:degree, cv.errs, xlab = 'Degree', ylab = 'Test MSE', type = 'l')
deg.min <- which.min(cv.errs)
points(deg.min, cv.errs[deg.min], col = 'red', cex = 2, pch = 19)
```

The minimum of test MSE at the degree 9. But test MSE of degree 4 is small enough.
The comparison by ANOVA (`anova(fit.1,fit.2,fit.3,fit.4,fit.5)`, on page 290, section 7.8.1) suggests degree 4 is enough.

Predict with 3 degree model:
```{r}
plot(wage ~ age, data = Wage, col = "darkgrey")
age.range <- range(Wage$age)
age.grid <- seq(from = age.range[1], to = age.range[2])
fit <- lm(wage ~ poly(age, 3), data = Wage)
preds <- predict(fit, newdata = list(age = age.grid))
lines(age.grid, preds, col = "red", lwd = 2)
```

## (b)

```{r}
cv.errs <- rep(NA, degree)
for (i in 2:degree) {
  Wage$age.cut <- cut(Wage$age, i)
  fit <- glm(wage ~ age.cut, data = Wage)
  cv.errs[i] <- cv.glm(Wage, fit)$delta[1]
}
plot(2:degree, cv.errs[-1], xlab = 'Cuts', ylab = 'Test MSE', type = 'l')
deg.min <- which.min(cv.errs)
points(deg.min, cv.errs[deg.min], col = 'red', cex = 2, pch = 19)
```

So 8 cuts produce minimum test MSE.

Predict with 8-cuts step function:
```{r}
plot(wage ~ age, data = Wage, col = "darkgrey")
fit <- glm(wage ~ cut(age, 8), data = Wage)
preds <- predict(fit, data.frame(age = age.grid))  # both `data.frame` and `list` work
lines(age.grid, preds, col = "red", lwd = 2)
```

Understand the `cut()` function:
```{r}
res <- cut(c(1,5,2,3,8), 2)
res
length(res)
class(res[1])
```

`cut(x, k)` acts like *bin* or *binage*, turning a continuous quantitative variable into a discrete qualitative variable, by deviding the range of `x` evenly into `k` intervals.
Each interval is called a *level*.
The output of `cut(x, k)` is a vector with the same length of `x`.
Each element of output (a *factor* object) is a *level* where the corresponding input element falls in.

# Question 7

See introductions about regression on qulitative predictors in section 3.3.1 and 3.6.6.

Use `summary()`
```{r}
set.seed(1)
summary(Wage$maritl)
# table(Wage$maritl) the same with `summary`
summary(Wage$jobclass)
par(mfrow = c(1, 2))
plot(Wage$maritl, Wage$wage)
plot(Wage$jobclass, Wage$wage)
```

Fit wage on multiple predictors with GAM:
```{r}
library(gam)
fit1 <- gam(wage ~ lo(year, span = 0.7) + s(age, 5) + education, data = Wage)
fit2 <- gam(wage ~ lo(year, span = 0.7) + s(age, 5) + education + jobclass, data = Wage)
fit3 <- gam(wage ~ lo(year, span = 0.7) + s(age, 5) + education + maritl, data = Wage)
fit4 <- gam(wage ~ lo(year, span = 0.7) + s(age, 5) + education + jobclass + maritl, data = Wage)
anova(fit1, fit2, fit3, fit4)
```

So model *fit4* fits the best.

Plot the model:
```{r}
par(mfrow = c(2, 3))
plot(fit4, se = T, col = "blue")
```

# Question 8

There's no direct way to answer if there are nonlinear realtionships between any feature in the data set. We have to choose a feature as the response, some others as predictors. And investigate if the relationship between them is linear or not.

Get the overall relationship between all pairs of the *Auto* data set:
```{r}
set.seed(1)
pairs(Auto)
```

From the plot above we can see when using *mpg* as the response, there are clear relationships between it and *cylinders*, *displacement*, *horsepower*, *weight*. Now we will find out if they are nonlinear or not.

```{r}
fit <- lm(mpg ~ poly(cylinders, 2) + poly(displacement, 5) + poly(horsepower, 5) + poly(weight, 5), data = Auto)
summary(fit)
```

The results show that there's no significant relationship between mpg and cylinders.
There's weak relationship (p-value: 0.028) between mpg and displacement.
There's strong quadratic relation between mpg and horsepower.
There's strong linear relation between mpg and weight.

Test the degree with ANOVA:
```{r}
anv1 <- gam(mpg ~ displacement + horsepower + weight, data = Auto)
anv2 <- gam(mpg ~ displacement + s(horsepower, 2) + weight, data = Auto)
anv3 <- gam(mpg ~ s(displacement, 5) + s(horsepower, 5) + s(weight, 5), data = Auto)
anova(anv1, anv2, anv3, test = 'F')
summary(anv3)
par(mfrow=c(1,3))
plot.Gam(anv3, se=TRUE, col="red")
```

According to plot of *anv3*, try quadratic with *displacement* and *horsepower*, linear with *weight*:
```{r}
anv4 <- gam(mpg ~ s(displacement, 3) + s(horsepower, 3) + weight, data = Auto)
anova(anv4, anv3, test = 'F')
par(mfrow=c(1,3))
plot(anv4, se=TRUE, col="red")
```

So model *anv4* is good enough.

Compare their test MSE:
```{r}
lm1 <- glm(mpg ~ displacement + horsepower + weight, data = Auto)
lm2 <- glm(mpg ~ poly(displacement, 3) + poly(horsepower, 3) + weight, data = Auto)
lm3 <- glm(mpg ~ poly(displacement, 5) + poly(horsepower, 5) + poly(weight, 5), data = Auto)

cv.glm(Auto, lm1, K = 10)$delta[1]
cv.glm(Auto, lm2, K = 10)$delta[1]
cv.glm(Auto, lm3, K = 10)$delta[1]
```

The results also suggest model *lm2* (same with *anv4*) is good enough.

So the conclusion of relationships with *mpg*:
mpg ~ displacement: cubic;
mpg ~ horsepower: cubic;
mpg ~ weight: linear.