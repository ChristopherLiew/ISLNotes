---
title: "第3章笔记"
output: html_notebook
---


线性回归是最基本的有监督学习方法。

第二章关于 广告 数据的一些问题：
* 广告预算和产品销量之间是否有关系？

* 二者之间的关系有多强？

* 三种广告渠道对产品销量是否有影响？

* 三种广告渠道对产品销量影响大小如何？每一元投入分别能产生多少产出？

* 能否基于现有数据预测未来销量？

* 广告预算和销量之间的关系是否可以看作线性关系？

* 不同广告媒体之间是否存在协同关系？

后面3.4节给出解答。

# 3.1 简单线性回归

## 3.1.1 参数估计

简单线性回归 **认为** 特征和响应变量间存在线性关系，见式 (3.1)，拟合线性模型的过程就是求解 $\beta_0$ 和 $\beta_1$ 的过程。

字母上加帽子（例如 $\hat y$）表示该量的估计值。
$\beta_0$ 和 $\beta_1$ 的过程。
求$\beta_0$ 和 $\beta_1$ 的方法是拟合，最基本的拟合方法是 最小二乘 法。
參差 RSS 的定义见式 (3.3)，图示见图 3.1。

式 (3.3) 分别对 $\hat \beta_0$ 和 $\hat \beta_1$ 求导得到二者的估计值，见式 (3.4)。

图 (3.2)，RSS 随 $\beta_0$ 和 $\beta_1$ 变化的图示。

## 3.1.2 参数计算精度评估

总体回归线（population regression line）是 **真实** 线性关系，最小二乘估计值是基于现有数据的计算结果，二者的关系见图 (3.3)，红线是总体回归线（理论上的正确值），蓝线是估计线。二者之间有差异的原因在于观测样本并不包括总体的所有信息，或者叫做通过样本推断总体，例如总体均值和样本均值并不相同，但样本均值是总体均值的一个好的估计值。

真实值和估计值之差叫做偏差 (bias)，如果所有估计值的均值等于真实值，叫做 无偏估计。
估计值与真实值的差距用估计值的标准差（$SE(\hat \mu)$）衡量，见式 (3.7)。
通过此式可知，对于相同分散度（方差）的样本，观测数越多（$n$ 越大），估计的精确度越高（$Var(\hat \mu)$越小）。

式 (3.8) 给出两个参数的计算公式，其中响应变量的方差就是扰动项的方差（$\sigma ^ 2 = Var(\epsilon)$）。

### 常用统计量

Variance of population，总体方差: $$\sigma ^ 2 = \frac{\sum{(x - \mu) ^ 2}}{n}$$

Variance of samples，样本方差: $$s ^ 2 = \frac{\sum ^ {n} _ {i = 1} (x_i - \bar x) ^ 2}{n - 1}$$

Standard deviation（$\sigma$，SD，标准差）是方差 $\sigma^2$ (variance) 的平方根，表示一组数据的 **分散** 程度，即这组数据中最大最小值与均值的距离大小；

Standard error（SE, 标准误差）是通过统计量估计总体时，样本分布的标准差，也可以理解为样本统计量的 **误差** 大小；

Standard error of sample mean（SEM，样本均值作为一个随机变量的SD）: $$\frac{s}{\sqrt n}$$

对样本数据的两种描述方法：
* 样本均值（sample mean）+ 样本标准差（standard deviation of sample data)
* 样本均值（sample mean）+ 样本均值标准误差（standard error of the sample mean)

Residual standard error定义：
$$
RSE = \sqrt{\frac{RSS}{n - 2}}
$$

模型 95% 置信区间：式 3.9 ~ 3.11，即期望左右两个方差。

### SE 用于假设检验

基于 p-value 的假设检验方法（反证法）：
假设 $H_0$ 成立，$\beta_1$ 服从 $t$ 分布，按此分布算出的统计量如果出现的几率太小，说明原假设不成立。

式 (3.14) 上面的一段话给出了 t 统计量计算公式的直观解释。
另外结合$H_0: \; \beta_1  = 0$，上面一节中 SEM 的计算公式（$SEM = s / \sqrt n$），以及单样本 t 统计量计算公式：
$$
t = \frac{\bar x - \mu_0}{\frac{s}{\sqrt n}}
$$
也能推出式 (3.14) 。

表 3.1 中数值关系：7.0325 / 0.4578 = 15.32, 0.0475 / 0.0027 = 17.59

## 3.1.3 模型精度评估

标示模型与数据 **拟合程度** 的两个值：$RSE$ 和 $R ^ 2$。

$RSE$ 是扰动项 $\epsilon$ 的标准差，也是响应变量的实际值相对于理论值的分散程度，与响应变量有相同的量纲。此值越小，说明预测越准确。
式 (3.16) 下面的一段话解释了表 3.2 中 $RSE = 3.26$ 的实际意义。

### $R ^ 2$ 的含义

式 (3.17) 中，$TSS$ 表示响应变量自身的方差（分散程度），与回归计算无关。
$RSS$ 表示无法通过回归计算解释的方差（响应变量实际值和响应值差的平方和），
所以 $R ^ 2$ 回归可以解释的方差在总方差中的比重，是个比例值，

$R ^ 2$ 接近1表示回归模型很好解释了响应变量的变化，接近0则表示回归模型对 $Y$ 的解释很差，
原因可能是模型不准确，或者扰动项 $\epsilon$ 确实很大，或者二者都有。

某些物理模型中，我们知道模型是线性的，这是哪怕 $R ^ 2$ 很高，但没有很接近1，都预示着严重问题的可能性。
而在其他心理学、生物学、商业模型中，本来就是非线性关系，当用线性模型拟合时，$R ^ 2$ 很小反而是合理的。

单变量回归模型中 $R^2$ 与 相关系数的平方是相等的（证明见本章习题7）。
多变量回归模型中无法实现相关系数，仍然用$R^2$作为评价指标。

# 3.2 多元线性回归

为什么对每个影响因素分别做单变量线性回归是不可取的？
* 无法做预测；
* 无法体现特征间的相互影响。

## 3.2.1 参数估计

多元线性回归的 $RSS$ 计算公式：(3.22)。

表 3.4 和 表 3.3 中 newspaper 系数（$\beta_1$）变化很大，原因是表 3.3 中的系数表示不考虑 电视和广播的影响，假设所有销售量的变化都是报纸引起的。
而表3,4中的系数表示当电视和广播广告费用不变时，报纸广告对销售量的影响。

根据表3.5说明单变量线性模型和多变量线性模型在报纸作用上给出相反结论的原因：
因为报纸和广播相关性高，报纸本身对销售额没有影响，但相关的广播广告费的增加导致了销售量上升，
在 *sales ~ newspaper* 单变量模型中，由于不考虑广播的作用，所以广播产生的效果被安在了报纸上。
但在多变量模型中，由于考虑了广播的作用，所以正确计算出报纸对销售量没有影响。

如果只考虑海滩上冰激凌销量与鲨鱼攻击事件的关系，会发现冰激凌销量的增加导致鲨鱼攻击事件的增加，
所以禁止海滩上销售冰激凌就可以降低鲨鱼攻击事件的数量。
但实际情况是温度升高导致更多人来到海滩上，导致冰激凌销量升高和鲨鱼攻击事件增多，
所以将温度纳入到鲨鱼攻击事件的特征中（另一个特征是冰激凌销量），就可以发现冰激凌销量和鲨鱼攻击事件之间并没有相关性。

## 3.2.2 几个重要问题

### 特征与响应变量之间是否有关系？

用 F-statistic 对应的 p-value 作为假设检验的依据（F-statistic 用于比较两个服从正态分布的总体的方差是否相等）。
若 p-value 小于 0.05，说明至少有一个特征与相应变量有关系（$\beta_1, \cdots, \beta_p$ 中至少有一个不为0）。

当 $p \gt n$ 时，不能使用 F-statistic 作为判断依据，应该使用什么方法，见第6章。

### 哪些特征最重要？

特征选择在第6章中详细介绍，这里简单提到几种方法，包括向前、向后、混合查找等。

### 模型拟合

一个对相应变量有影响的特征，会导致 $R^2$ 升高，$RSE$ 降低，反之一个与相应变量关系不大的变量则对两个量的影响不大。

### 预测

得到一组参数值（$\hat\beta_0, \hat\beta_1, \cdots, \hat\beta_p$）后，代入式 (3.21) 中就可以预测响应值了。
预测区间（prediction interval）用于表示响应变量的估计值（$\hat Y$）与实际值（$Y$）之间的差距。
置信区间（confidence interval）用于表示响应变量均值可能存在的区间，或者叫可消除误差范围，即模型参数的估计值（$\hat Y = \hat\beta_0 + \hat\beta_1 X_1 + \cdots + \hat\beta_p X_p$）与理论值（$f(X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p$）之间的误差范围。
预测区间则可以表示某个具体值存在的区间，预测区间总是大于置信区间，因为前者包含了可消除误差和不可消除误差两部分，而均值可以去掉不可消除误差引入的不确定性（$\because E(\epsilon) = 0$）。

# 3.3 回归模型的几个问题

## 3.3.1 类别型特征

创建一个 dummy variable 代表一个二分的类别型变量，例如性别，如果某类别型变量的可选值有 $n \; (n \gt 2)$ 个，则需要创建 $n - 1$个 dummy variables.

## 3.3.2 拓展线性模型
多元线性模型的两个假设：
* 可加性：各个特征对响应的影响彼此独立，互不影响；
* 线性关系：特征变量每变化一个单位引起响应的变化是恒定的，与特征的大小无关；

### 去掉可加性假设
图3.5：协同作用（synergy effect）、交互作用（interaction effect）图示：在TV和Radio两个坐标轴的角平分线上，线性模型低估了实际销售额（绿色平面低于红色点），表明TV和Radio之间存在交互作用。

继承原则（hierarchical principle）：如果考虑特征的交互作用，就必须也把参与交互作用的特征也考虑进来，不论特征对于响应是否显著。

图3.7：类别型特征和数值型特征之间的交互作用，左图表示无交互作用，右图表示有交互作用：随着收入的增加，学生和非学生的账单金额差距在缩小。

### 去掉线性假设

多项式回归：通过增加高次方项（例如最高为二次方的二次型系统），实现用 **线性模型** 的方式拟合数据。多项式回归仍然是一种线性模型，这就是为什么 R 中多项式回归和线性回归都采用 `lm()` 函数计算。

多项式回归采用多高的次数合适？在数据探索过程中，如果阶数增加后，$R^2$增加不明显，或者 p-value 大于 0.05，则不应该增加此高次项。

## 3.3.3 潜在问题
如何解决线性模型中的问题，没有统一的处理答案，
具体如何操作，与其说是技术，不如说是艺术。

### 1. 非线性关系
如何判断线性模型是否能够比较好的解释观测数据？

使用 residual plot，在单变量线性回归场景下，用特征 $X$ 作为横轴，残差作为纵轴；在多变量线性回归场景下，使用相应变量的估计值 $\hat y$ 作为横轴，残差作为纵轴：`plot(predict(lm.fit), residuals(lm.fit))`（见p112, 113）。

如果图中的点具有某种模式，说明残差中仍然有信息，线性模型不能很好的解释数据。

### 2. 扰动项彼此不独立

线性回归假设各个观测的扰动项之间彼此独立，如果这个假设不成立，会导致参数估计的置信区间变窄，也就是高估了参数的可信度。
例如，假设我们有一个包含100个观测的数据集，全部乘2得到新的100个观测，然后新观测数据与原来的100个观测放在一起，基于这200个观测得到的参数估计与基于原来100个得到的参数是一样的（因为增加的100个观测并没有提供新信息），但参数的置信区间宽度只有原来的71%（$1 / \sqrt 2$）。

为什么扰动项之间会存在相关性？
扰动项的相关性经常出现在时序数据中，为了识别一列时序数据的扰动项是否存在相关性，可以绘制扰动项随时间变化图，并观测其中是否存在规律（见图3.10），另外ACF 和 PACF 图能更精确地描绘出扰动项的相关性。

除了时序数据，其他场景中也可能存在类似问题，例如分析身高和体重的关系时，如果观测值来自于同一个家庭，或者有相同的饮食习惯，或者生活环境高度一致，都可能产生扰动项的相关性。

由于线性回归高度依赖扰动项彼此独立的假设，所以设计试验时必须注意避免出现扰动项不独立的情况。

### 3. 扰动项的非稳态方差
线性回归基于 同方差（homoscedasticity）假设。
异方差性（heteroscedasticity）：残差图有漏斗形状，例如图3.11左图。
当响应变量存在异方差性时，对其施加凹函数（例如 $\log Y$ 或者 $\sqrt Y$）以降低其异方差程度，例如图3.11右图。
当已知方差分布规律时，可以使用加权最小二乘法（weighted least sqares）拟合线性模型。

### 4. 响应异常值
响应异常值（outlier）指实际值 $y_i$ 与模型预测值 $\hat y_i$ 差距很大的观测值。
造成 outlier 的原因很多，例如设备故障、录入错误等。

outlier 一般不会对模型参数造成很大影响，但对模型 $R^2$ 和 RSE 影响较大。
一般使用 studentized residuals 作为 outlier 的评价指标，见图3.12。

### 5. 特征异常值
参考[特征异常平滑](https://github.com/znbt/mlOperatorIntro/blob/master/tutorial/anomalyFeature.Rmd)。
R中常用 `hatvalues()` 函数作为特征异常值检测工具，例如 p113 `plot(hatvalues(lm.fit))`。

### 6. 共线性

为什么图3.15右图具有更高的共线性？

数据的微小变动会导致模型参数在“峡谷”方向上大幅度变化。
存在共线性的模型参数的变化幅度大大高于不存在共线性的模型幅度（$0.03 \sim 0.4$, 这里 $0.03 = 0.19 - 0.16$，$0.4 = 0.2 - (- 0.2)$）。

共线性对模型的影响实例：表3.11

假设自变量 $x_{i1}$ 和 $x_{i3}$ 线性相关，也就是二者存在 *collinearity*，Collinearity 导致原本对结果变量 $y_i$ 有影响的 $x_{i1}$ 被 $x_{x3}$ 影响，计算结果显示 $x_{i1}$ 对 $y_i$ 无影响。在 *Table 3.11* 中，*Model 2* 中 *rating* 和 *limit* 线性相关，*limit* 对 *Balance* 是有影响的（*Model 1* 中 *limit* 的 *p-value* 小于 0.0001），但 Model 2 的计算结果显示 *limit* 对 *Balance* 无影响（*p-value* = 0.7012），所以我们说 *limit* 被 *rating* 掩盖 (mask) 了。

检查是否存在共线性：通过相关性矩阵可以检验任何两个特征间是否存在相关，但相关性矩阵无法检测到多元共线性；使用 VIF 计算 **每个** 特征的共线性，VIF最小值为1，越大表示共线性可能性越大，一般 VIF 大于 5 ~ 10 就属于共线性比较严重的情况了。

处理共线性：发现共线性的特征后，最简单的方法是去掉其中一个特征，或者将相关的特征合并为一个特征。

# 3.5 线性模型与knn方法的比较

参数模型和无参数模型各自的优缺点。

KNN：$K$ 值越小，bias 越小，variance 越大，$K$ 值越大，bias 越大，variance 越小。第5章介绍的方法用于确定最优 $K$ 值。

当实际数据关系接近参数模型所选择的模型时，参数模型优于非参数模型。
图 3.17, 3.18 对比了当实际关系为线性时，KNN与线性回归模型，后者在计算成本和拟合效果两方面都好于前者。
图 3.19 表明高度非线性场景中 KNN 会由于 线性回归。

图 3.20：非线性场景中，随着特征数量 $p$ 的增加，线性回归预测误差缓慢上升，KNN预测误差则快速上升。

总原则是，$n / p$ 越小，越倾向于选择参数模型，即使 $n / p$ 较大，只要线性模型的性能不太差，也应该优先选择参数模型，因为它的可解释性好，可以提供 p-value 等评价指标。
