---
title: "Applied Exercises of Chapter 8"
output: html_notebook
---

# Question 7

In the lab, we applied random forests to the *Boston* data using `mtry=6` and using `ntree=25` and `ntree=500`. Create a plot displaying the test error resulting from random forests on this data set for a more comprehensive range of values for `mtry` and `ntree`. You can model your
plot after Figure 8.10. Describe the results obtained.

```{r}
library(MASS)
library(randomForest)
set.seed(1)
train <- sample(1: nrow(Boston), nrow(Boston) / 2)
boston.test <- Boston[-train, 'medv']
p <- ncol(Boston) - 1

tree.nos <- c(1, 2, 3, 4, seq(5, 500, by = 5))

rf.gen.err <- function(tree.no, feature.no) {
  rf.boston <- randomForest(medv ~ ., data = Boston, subset = train, 
                            mtry = feature.no, ntree = tree.no, importance = TRUE)
  yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
  mean((yhat.rf - boston.test) ^ 2)
}

err.full <- sapply(tree.nos, rf.gen.err, feature.no = p)
err.half <- sapply(tree.nos, rf.gen.err, feature.no = round(p/2))
err.sqrt <- sapply(tree.nos, rf.gen.err, feature.no = round(sqrt(p)))
```

Plot the errors with different feature numbers and tree numbers:
```{r}
plot(tree.nos, err.full, type = 'l', col = 'red', ylim = c(10, 24.5), xlab = 'Number of trees', ylab = 'Test MSE')
lines(tree.nos, err.half, type = 'l', col = 'blue')
lines(tree.nos, err.sqrt, type = 'l', col = 'darkgreen')
legend("topright", legend = c("m=p", "m=p/2", 'm=sqrt(p)'), col = c('red', 'blue', 'darkgreen'), lty = 1, lwd =2, cex = 0.8)
```

Get the errors from the function itself:
```{r}
Boston.train <- Boston[train, -14]
Boston.test <- Boston[-train, -14]
Y.train <- Boston[train, 14]
Y.test <- Boston[-train, 14]
rf.boston.full <- randomForest(Boston.train, y = Y.train, xtest = Boston.test, ytest = Y.test, mtry = ncol(Boston) - 1, ntree = 500)
rf.boston.half <- randomForest(Boston.train, y = Y.train, xtest = Boston.test, ytest = Y.test, mtry = (ncol(Boston) - 1) / 2, ntree = 500)
rf.boston.sqrt <- randomForest(Boston.train, y = Y.train, xtest = Boston.test, ytest = Y.test, mtry = sqrt(ncol(Boston) - 1), ntree = 500)
plot(1:500, rf.boston.full$test$mse, col = "red", type = "l", xlab = "Number of Trees", ylab = "Test MSE", ylim = c(10, 19))
lines(1:500, rf.boston.half$test$mse, col = "blue", type = "l")
lines(1:500, rf.boston.sqrt$test$mse, col = "darkgreen", type = "l")
legend("topright", c("m = p", "m = p/2", "m = sqrt(p)"), col = c("red", "blue", "darkgreen"), cex = 1, lty = 1)
```

For feature numbers, half (blue) and square root (dark green) of all predictors are almost the same.
They are both better than using all features.

# Question 8

In the lab, a classification tree was applied to the *Carseats* data set after converting *Sales* into a qualitative response variable. Now we will seek to predict *Sales* using regression trees and related approaches, treating the response as a quantitative variable.

(a) and (b): build a complete regression tree on training set of *Carseats* and calculate its test MSE:
```{r}
library(ISLR)
library(tree)
set.seed(1)
train <- sample(1 : nrow(Carseats), nrow(Carseats) / 2)
tree.carseats <- tree(Sales ~ ., data = Carseats, subset = train)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
tree.pred <- predict(tree.carseats, Carseats[-train, ])
mean((Carseats[-train, 'Sales'] - tree.pred) ^ 2)
```

The test MSE of complete regression tree is 4.15.

(c): build a pruning tree on training set and calculate its test MSE:
```{r}
set.seed(1)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.tree)
tree.min <- which.min(cv.carseats$dev)
best.node.no <- cv.carseats$size[tree.min]

prune.carseats <- prune.tree(tree.carseats, best = best.node.no)
tree.pred <- predict(prune.carseats, Carseats[-train, ])
mean((Carseats[-train, 'Sales'] - tree.pred) ^ 2)
```

The test MSE of pruning tree is higher than complete tree.
So pruning the tree doesn't prove the test MSE.

Note: use `prune.misclass` for pruning the classification tree, `prune.tree` for regression tree.

Plot the node number and pruning tree:
```{r}
plot(cv.carseats$size, cv.carseats$dev, type = "b")
points(best.node.no, cv.carseats$dev[tree.min], col = "red", cex = 2, pch = 20)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

(d): calculate the test MSE with bagging approach:
```{r}
library(randomForest)
set.seed(1)
bag.carseats <- randomForest(Sales ~ ., data = Carseats, subset = train, mtry = ncol(Carseats) - 1, importance = TRUE)
yhat.bag <- predict(bag.carseats, newdata = Carseats[-train, ])
mean((yhat.bag - Carseats[-train, 'Sales']) ^ 2)
```

The test MSE is 2.615.
The importance of predictors:
```{r}
importance(bag.carseats)
```

So the most 2 important predictors are: *Price* and *ShelveLoc*.

(e): Calculate test MSE with random forests and different parameter *m*:
```{r}
rf.carseats <- randomForest(Sales ~ ., data = Carseats, subset = train, importance = TRUE)
yhat.rf <- predict(rf.carseats, Carseats[-train, ])
mean((yhat.rf - Carseats[-train, 'Sales']) ^ 2)
importance(rf.carseats)
```

The default *m* for a regression problem is $p/3$, where *p* is number of predictors. In this setting, the test MSE is 3.321, higher than bagging method, lower than complete and pruning regression tree.

The most 2 important predictors are the same with bagging method above: *Price* and *ShelveLoc*.
