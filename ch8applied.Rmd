---
title: "Applied Exercises of Chapter 8"
output: html_notebook
---

# Question 7

In the lab, we applied random forests to the *Boston* data using `mtry=6` and using `ntree=25` and `ntree=500`. Create a plot displaying the test error resulting from random forests on this data set for a more comprehensive range of values for `mtry` and `ntree`. You can model your
plot after Figure 8.10. Describe the results obtained.

```{r}
library(MASS)
library(randomForest)
set.seed(1)
train <- sample(1: nrow(Boston), nrow(Boston) / 2)
boston.test <- Boston[-train, 'medv']
p <- ncol(Boston) - 1

tree.nos <- c(1, 2, 3, 4, seq(5, 500, by = 5))

rf.gen.err <- function(tree.no, feature.no) {
  rf.boston <- randomForest(medv ~ ., data = Boston, subset = train, 
                            mtry = feature.no, ntree = tree.no, importance = TRUE)
  yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
  mean((yhat.rf - boston.test) ^ 2)
}

err.full <- sapply(tree.nos, rf.gen.err, feature.no = p)
err.half <- sapply(tree.nos, rf.gen.err, feature.no = round(p/2))
err.sqrt <- sapply(tree.nos, rf.gen.err, feature.no = round(sqrt(p)))
```

Plot the errors with different feature numbers and tree numbers:
```{r}
plot(tree.nos, err.full, type = 'l', col = 'red', ylim = c(10, 24.5), xlab = 'Number of trees', ylab = 'Test MSE')
lines(tree.nos, err.half, type = 'l', col = 'blue')
lines(tree.nos, err.sqrt, type = 'l', col = 'darkgreen')
legend("topright", legend = c("m=p", "m=p/2", 'm=sqrt(p)'), col = c('red', 'blue', 'darkgreen'), lty = 1, lwd =2, cex = 0.8)
```

Get the errors from the function itself:
```{r}
Boston.train <- Boston[train, -14]
Boston.test <- Boston[-train, -14]
Y.train <- Boston[train, 14]
Y.test <- Boston[-train, 14]
rf.boston.full <- randomForest(Boston.train, y = Y.train, xtest = Boston.test, ytest = Y.test, mtry = ncol(Boston) - 1, ntree = 500)
rf.boston.half <- randomForest(Boston.train, y = Y.train, xtest = Boston.test, ytest = Y.test, mtry = (ncol(Boston) - 1) / 2, ntree = 500)
rf.boston.sqrt <- randomForest(Boston.train, y = Y.train, xtest = Boston.test, ytest = Y.test, mtry = sqrt(ncol(Boston) - 1), ntree = 500)
plot(1:500, rf.boston.full$test$mse, col = "red", type = "l", xlab = "Number of Trees", ylab = "Test MSE", ylim = c(10, 19))
lines(1:500, rf.boston.half$test$mse, col = "blue", type = "l")
lines(1:500, rf.boston.sqrt$test$mse, col = "darkgreen", type = "l")
legend("topright", c("m = p", "m = p/2", "m = sqrt(p)"), col = c("red", "blue", "darkgreen"), cex = 1, lty = 1)
```

For feature numbers, half (blue) and square root (dark green) of all predictors are almost the same.
They are both better than using all features.

# Question 8

In the lab, a classification tree was applied to the *Carseats* data set after converting *Sales* into a qualitative response variable. Now we will seek to predict *Sales* using regression trees and related approaches, treating the response as a quantitative variable.

(a) and (b): build a complete regression tree on training set of *Carseats* and calculate its test MSE:
```{r}
library(ISLR)
library(tree)
set.seed(1)
train <- sample(1 : nrow(Carseats), nrow(Carseats) / 2)
tree.carseats <- tree(Sales ~ ., data = Carseats, subset = train)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
tree.pred <- predict(tree.carseats, Carseats[-train, ])
mean((Carseats[-train, 'Sales'] - tree.pred) ^ 2)
```

The test MSE of complete regression tree is 4.15.

(c): build a pruning tree on training set and calculate its test MSE:
```{r}
set.seed(1)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.tree)
tree.min <- which.min(cv.carseats$dev)
best.node.no <- cv.carseats$size[tree.min]

prune.carseats <- prune.tree(tree.carseats, best = best.node.no)
tree.pred <- predict(prune.carseats, Carseats[-train, ])
mean((Carseats[-train, 'Sales'] - tree.pred) ^ 2)
```

The test MSE of pruning tree is higher than complete tree.
So pruning the tree doesn't prove the test MSE.

Note: use `prune.misclass` for pruning the classification tree, `prune.tree` for regression tree.

Plot the node number and pruning tree:
```{r}
plot(cv.carseats$size, cv.carseats$dev, type = "b")
points(best.node.no, cv.carseats$dev[tree.min], col = "red", cex = 2, pch = 20)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

(d): calculate the test MSE with bagging approach:
```{r}
library(randomForest)
set.seed(1)
bag.carseats <- randomForest(Sales ~ ., data = Carseats, subset = train, mtry = ncol(Carseats) - 1, importance = TRUE)
yhat.bag <- predict(bag.carseats, newdata = Carseats[-train, ])
mean((yhat.bag - Carseats[-train, 'Sales']) ^ 2)
```

The test MSE is 2.615.
The importance of predictors:
```{r}
importance(bag.carseats)
```

So the most 2 important predictors are: *Price* and *ShelveLoc*.

(e): Calculate test MSE with random forests and different parameter *m*:
```{r}
rf.carseats <- randomForest(Sales ~ ., data = Carseats, subset = train, importance = TRUE)
yhat.rf <- predict(rf.carseats, Carseats[-train, ])
mean((yhat.rf - Carseats[-train, 'Sales']) ^ 2)
importance(rf.carseats)
```

The default *m* for a regression problem is $p/3$, where *p* is number of predictors. In this setting, the test MSE is 3.321, higher than bagging method, lower than complete and pruning regression tree.

The most 2 important predictors are the same with bagging method above: *Price* and *ShelveLoc*.

# Question 9

(a) and (b): Split the OJ data set for training and test, and fit a classification tree.
Note that there is no *Buy* feature in this data set.
```{r}
library(ISLR)
library(tree)
set.seed(1)
train <- sample(1 : nrow(OJ), 800)
oj.train <- OJ[train, ]
oj.test <- OJ[-train, ]
oj.tree <- tree(Purchase ~ ., OJ, subset = train)
summary(oj.tree)
```

There are 4 features used in tree construction: "LoyalCH", "PriceDiff", "SpecialCH", "ListPriceDiff".
The error rate on training set is 0.165. The number of terminal nodes is 8.

(c): Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.

```{r}
oj.tree
```

As an example, terminal node no. 20 contains 70 observations, whose *LoyalCH* feature is in range [0.264232, 0.508643], and *PriceDiff* less than 0.195, and *SpecialCH* less than 0.5. The $Y$ of this node is *MM*, which means purchases in this node are all "Minute Maid" orange juice bought by customers. In this node, 15.714% of $Y$ are "CH", and 84.286% of $Y$ are "MM".

What does *deviance* in the output mean?
How does the *deviance* of node 20 (60.89) is calculated?

(d): Create a plot of the tree, and interpret the results.
```{r}
plot(oj.tree)
text(oj.tree)
```

(e): Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?

```{r}
oj.pred <- predict(oj.tree, oj.test, type = 'class')
table(oj.pred, oj.test$Purchase)
```

The test error rate is:
```{r}
(12 + 49) / (147 + 49 + 12 + 62)
```

(f): Apply the `cv.tree()` function to the training set in order to determine the optimal tree size.
```{r}
oj.cv <- cv.tree(oj.tree, FUN = prune.misclass)
oj.cv
```

The optimal tree size is 2, which has the same *dev* value (156) of the tree with 8 nodes.
*dev* means cross-validated classification error rate of the tree.

(g) and (h): Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.

```{r}
plot(oj.cv$size, oj.cv$dev, xlab="Size of the Tree", ylab="CV error rate", type = "b")
points(2, oj.cv$dev[2], col = "red", cex = 2, pch = 20)
```

Tree size 2, 5 and 8 have the same cross-validation classification error rate.

(i): Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.

```{r}
oj.pruned <- prune.misclass(oj.tree, best = 2)
oj.pruned
plot(oj.pruned)
text(oj.pruned)
```

(j): Compare the training error rates between the pruned and unpruned trees. Which is higher?

```{r}
summary(oj.pruned)
```

The training error rate of pruned tree (0.1825) is higher than that of complete tree (0.165).

(k): Compare the test error rates between the pruned and unpruned trees. Which is higher?
```{r}
oj.pred <- predict(oj.tree, oj.test, type = 'class')
table(oj.pred, oj.test$Purchase)

oj.pruned.pred <- predict(oj.pruned, oj.test, type = 'class')
table(oj.pruned.pred, oj.test$Purchase)
```

The test error rate is:
```{r}
(40 + 30) / (119 + 30 + 40 + 81)
```

The test error rate of pruned tree (0.259) is higher than that of complete tree (0.2259).

# Question 10

(a): Remove the observations for whom the salary information is unknown, and then log-transform the salaries.
```{r}
Hitters <- na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
```

(b): Create a training set consisting of the first 200 observations, and
a test set consisting of the remaining observations.
```{r}
train <- 1:200
hitter.train <- Hitters[train, ]
hitter.test <- Hitters[-train, ]
```

(c): Perform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter $\lambda$. Produce a plot with different shrinkage values on the x-axis and the corresponding training set MSE on the y-axis.
(d): Produce a plot with different shrinkage values on the x-axis and
the corresponding test set MSE on the y-axis.
```{r}
library(gbm)
set.seed(1)
shrks <- 10 ^ seq(-10, -0.1, by = 0.1)
train.err <- rep(NA, length(shrks))
test.err <- rep(NA, length(shrks))
for (i in 1 : length(shrks)) {
  hitter.boost <- gbm(Salary ~ ., data = hitter.train, distribution = 'gaussian', n.trees = 1000, shrinkage = shrks[i])
  train.pred <- predict(hitter.boost, hitter.train, n.trees = 1000)
  train.err[i] <- mean((train.pred - hitter.train$Salary) ^ 2)
  test.pred <- predict(hitter.boost, hitter.test, n.trees = 1000)
  test.err[i] <- mean((test.pred - hitter.test$Salary) ^ 2)
}
plot(shrks, train.err, type="l", xlab="Shrinkage", ylab="MSE", col="blue")
lines(shrks, test.err, type="b", xlab="Shrinkage", ylab="Test MSE", col="red", pch=20)
legend("topright", legend = c("training MSE", "test MSE"), col = c('blue', 'red'), lty = 1, lwd =2, cex = 0.8)
```

(e): Compare the test MSE of boosting to the test MSE that results from applying two of the regression approaches seen in Chapters 3 and 6.
```{r}
lm.fit <- lm(Salary ~ ., data = hitter.train)
lm.pred <- predict(lm.fit, hitter.test)
mean((lm.pred - hitter.test$Salary)^2)
library(glmnet)
set.seed(1)
x <- model.matrix(Salary ~ ., data = hitter.train)
y <- hitter.train$Salary
x.test = model.matrix(Salary ~ ., data = hitter.test)
lasso.fit <- glmnet(x, y, alpha = 1)
lasso.pred <- predict(lasso.fit, s = 0.01, newx = x.test)
mean((hitter.test$Salary - lasso.pred) ^ 2)
```

The test MSE of linear regression (0.49) and lasso (0.47) are both larger than that of boosted decision tree model.

(f): Which variables appear to be the most important predictors in the boosted model?

```{r}
summary(gbm(Salary ~ ., data = hitter.train, distribution = 'gaussian', n.trees = 1000, shrinkage = shrks[which.min(test.err)]))
```

*CAtBat*, *CHits* and *PutOuts* are the most important 3 predictors.

(g): Now apply bagging to the training set. What is the test set MSE for this approach?

```{r}
library(randomForest)
set.seed(1)
hitter.bag <- randomForest(Salary ~ ., data = Hitters, subset = train, mtry = ncol(Hitters) - 1, importance = TRUE)
hitter.pred <- predict(hitter.bag, hitter.test)
mean((hitter.pred - hitter.test$Salary) ^ 2)
min(test.err)
```

Test MSE of bagging (0.230) is a little lower than that of boosted tree (0.254).

# Question 11

(a): Create a training set consisting of the first 1,000 observations, and a test set consisting of the remaining observations.
(b): Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. Which predictors appear to be the most important?

```{r}
library(gbm)
set.seed(1)
Caravan$binPurchase <- rep(0, nrow(Caravan))
Caravan$binPurchase[Caravan$Purchase == 'Yes'] <- 1
train <- 1:1000
cara.train <- Caravan[train, ]
cara.test <- Caravan[-train, ]
cara.boosted <- gbm(binPurchase ~ . - Purchase, data = cara.train, distribution = 'bernoulli', n.trees = 1000, shrinkage = 0.01)
summary(cara.boosted)
```

*PPERSAUT*, *MKOOPKLA* and *MOPLHOOG* are most important predictors.

Note: use `distribution = 'gaussian'` for regression problem, and `distribution = 'bernoulli'` for classification problem. See "Arguments > distribution" in the documentation of `gbm` for details:

> either a character string specifying the name of the distribution to use or a list with a component name specifying the distribution and any additional parameters needed. If not specified, gbm will try to **guess**: if the response has only 2 unique values, **bernoulli** is assumed; otherwise, if the response is a factor, multinomial is assumed; otherwise, if the response has class "Surv", coxph is assumed; otherwise, **gaussian** is assumed.


(c): Use the boosting model to predict the response on the test data. Predict that a person will make a purchase if the estimated probability of purchase is greater than 20%. Form a confusion matrix. What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?
```{r}
cara.pred <- predict(cara.boosted, cara.test, n.trees = 1000, type = 'response')
summary(cara.test$Purchase[cara.pred > 0.2])
```

```{r}
33 / (123 + 33)
```

In all predicted buyers (whose predicted probability larger than 20%), there are 21% people do a purchase in fact.

Note: the section *Value* in documentation of `predict.gbm` explains why we need `type = 'response`` added:

> Returns a vector of predictions. By default the predictions are on the scale of f(x). For example, for the Bernoulli loss the returned value is on the **log odds scale**, poisson loss on the log scale, and coxph is on the log hazard scale.
If **type="response"** then gbm converts back to **the same scale as the outcome**. Currently the only effect this will have is returning probabilities for bernoulli and expected counts for poisson. For the other distributions "response" and "link" return the same.

Fit with linear model:
```{r}
cara.lm <- glm(binPurchase ~ . - Purchase, data = cara.train, family = 'binomial')
cara.pred <- predict(cara.lm, cara.test, type = 'response')
summary(cara.test$Purchase[cara.pred > 0.2])
58 / (350 + 58)
```

The correct rate of logistic regression is 14.2%, lower than that of boosted regression model.
