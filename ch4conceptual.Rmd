---
title: "Conceptual Exercises of Chapter 4"
output: html_notebook
---

# Question 1

Let $Y=e^{\beta_0 + \beta_1 X}$, we have:
$$
p(X) = \frac{Y}{1 + Y} \\
\therefore p(X) + Yp(X) = Y \\
p(X) = [1 - p(X)] Y \\
\frac{p(X)}{1 - p(X)} = Y = e^{\beta_0 + \beta_1 X}
$$

# Question 2

When proving the same $k$ produce both maximum $p_k(x)$ and maximum of $\delta_k(x)$,
we have assumed that $\sigma_1^2=\dots=\sigma_K^2$, the only variable is $k$ in equation (4.12) and (4.13). So let
$$
\frac { \frac {1} {\sqrt{2 \pi} \sigma} \exp(- \frac {1} {2 \sigma^2} x^2) } {\sum_{l=1}^K { \pi_l \frac {1} {\sqrt{2 \pi} \sigma} \exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2) }} = C
$$

Take it into equation (4.12) and (4.13), we have:
$$
p_k(x) = \exp (x \frac {\mu_k} {\sigma^2} - \frac {\mu_k^2} {2 \sigma^2}) \pi_k C \\
\therefore
log(p_k(x))
= x\frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + log(\pi_k) + log(C)
= \delta_k(x) + log(C)
$$

For logarithm function is monotonically increasing, when $\delta_k(x)$ get its maximum,
$p_k(x)$ get its maximum, too.

# Question 3

Like above question, but without assumption that $\sigma_1^2 = \dots = \sigma_K^2$,
let:
$$
\frac {\frac {1} {\sqrt{2 \pi}}} {\sum_{l=1}^K { \pi_l \frac {1} {\sqrt{2 \pi} \sigma} \exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2) }} = C
$$

Take it into equation (4.12) and (4.13), we have:
$$
p_k(x) = \exp(-\frac{(x - \mu_k)^2}{2 \sigma_k^2}) \frac{\pi_k}{\sigma_k} C \\
\therefore
log(p_k(x)) = \delta_k(x)
= -\frac{(x - \mu_k)^2}{2 \sigma_k^2} + log(\frac{\pi_k}{\sigma_k}) + log(C)
$$

So $\delta_k(x)$ is a quadratic function of $x$.

# Question 4

## 4a ~ 4d
0.1 (10% in other words), 0.01, and $10^{-100}$.
As the increase of $p$, the near points decrease exponentially.

## 4e
The length of each side for $p$ dimensional hypercube is $0.1^{\frac1p}$.

# Question 5

5a: When the Bayes decision boundary is linear, QDA performs better than QDA on training set.
LDA performs better than QDA on test set.

5b: When the Bayes boundary is non-linear, QDA performs better than LDA on both training and test sets.

5c: When $n$ increase, QDA predicts more accurately than LDA, because the bias of QDA decrease faster than LDA.
See the first paragraph of page 150 for reference.

5d: False. The bias of QDA can be smaller than LDA, which produces higher variance than LDA.
The higher variance produces higher error rate in test data. See figure 4.9 for reference.